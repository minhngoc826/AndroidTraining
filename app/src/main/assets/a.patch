Subject: [PATCH] patch-09-02-25
---
Index: app/src/main/assets/DevopsDoc/IpPortServer.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/app/src/main/assets/DevopsDoc/IpPortServer.txt b/app/src/main/assets/DevopsDoc/IpPortServer.txt
--- a/app/src/main/assets/DevopsDoc/IpPortServer.txt	(revision 33974a2fda3bd9b55281f0b074a63deb654aa747)
+++ b/app/src/main/assets/DevopsDoc/IpPortServer.txt	(date 1739117556855)
@@ -1,8 +1,8 @@
 ///////////////////	Home ////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
 Name						IP						Domain					Web/Port/Type
-lab-server					192.168.1.10			server.devops.com		todolist:8080/Vuejs:nginx		shoeshop
-git-db-registry-server		192.168.1.20			gitlab.devops.com		gitlab:80						database:3306				registry:5000
-jenkins-server				192.168.1.30			jenkins.devops.com		jenkins:8080
+lab-server					192.168.55.10			server.devops.com		todolist:8080/Vuejs:nginx		shoeshop
+git-db-registry-server		192.168.55.20			gitlab.devops.com		gitlab:80						database:3306				registry:5000
+jenkins-server				192.168.55.30			jenkins.devops.com		jenkins:8080
 ///////////////////	Company	//////////////////////////////////////////////////////////////////
 Name						IP						Domain					Web/Port/Type
 lab-server					192.168.56.10			server.devops.com		todolist:2001/Vuejs:nginx			vision:3000/React:service			shoeshop:8090/Spring:
@@ -22,6 +22,8 @@
 192.168.1.10 server.devops.com
 192.168.1.20 gitlab.devops.com
 192.168.1.30 jenkins.devops.com
+192.168.56.60 dev-server
+192.168.56.50 k8s-worker
 ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
 ssh ngocnm@192.168.56.
 192.168.56.10 server.devops.com
@@ -60,6 +62,11 @@
 		- VM:		sudo mount -t vboxsf SharedFolder /mnt/shared-folder
 		- Host: 	copy file to E:\VMWare\SharedFolder		-> ls /mnt/shared-folder
 		- VM:		cp filePath /mnt/shared-folder			-> F5 E:\VMWare\SharedFolder
+	+ shared folder VMWare:
+		- VM setting -> ennable shared folder	-> 	add folder: D:\DevOps\VMWare\SharedFolder
+		- Menu -> Edit -> Preferences: Enable all shared foldes by default
+		- sudo apt-get install open-vm-tools
+		- mkdir /mnt/shared-folder 		->		sudo /usr/bin/vmhgfs-fuse .host:/SharedFolder /mnt/shared-folder
 ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
 1. Project
 	- install dependencies / tools for project (run & deploy)
Index: app/src/main/assets/b.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/app/src/main/assets/b.txt b/app/src/main/assets/b.txt
--- a/app/src/main/assets/b.txt	(revision 33974a2fda3bd9b55281f0b074a63deb654aa747)
+++ b/app/src/main/assets/b.txt	(date 1739128690938)
@@ -1,6 +1,7 @@
 https://www.facebook.com/groups/950551346787852/

- sudo kubeadm init
+# sudo kubeadm init
+
 [sudo] password for devops:
 I0208 12:06:59.373375    1127 version.go:256] remote version is much newer: v1.32.1; falling back to: stable-1.30
 [init] Using Kubernetes version: v1.30.9
@@ -77,5 +78,7 @@

 Then you can join any number of worker nodes by running the following on each as root:

-kubeadm join 10.0.2.15:6443 --token hghp1j.1n11loiwd2l0me3g \
-        --discovery-token-ca-cert-hash sha256:1d985d8a88c0e6a2e456afaa2e2687425091faa411a3f1d1c20eae7e42541e4b
\ No newline at end of file
+IP: 192.168.56.70
+// 1 master - n worker
+kubeadm join 192.168.56.70:6443 --token arv434.r8yjaxhcf1eghmmn \
+        --discovery-token-ca-cert-hash sha256:2cf3ddb19c0e6c942ac5d965aab0fbf0a6264892fd86664f4b5c2cca6f482755
\ No newline at end of file
Index: app/src/main/assets/DevopsDoc/10.Cloud.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/app/src/main/assets/DevopsDoc/10.Cloud.txt b/app/src/main/assets/DevopsDoc/10.Cloud.txt
new file mode 100644
--- /dev/null	(date 1739047006422)
+++ b/app/src/main/assets/DevopsDoc/10.Cloud.txt	(date 1739047006422)
@@ -0,0 +1,1 @@
+AWS seriers:	https://www.youtube.com/@Viet100
\ No newline at end of file
Index: app/src/main/assets/DevopsDoc/0.DevOps.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/app/src/main/assets/DevopsDoc/0.DevOps.txt b/app/src/main/assets/DevopsDoc/0.DevOps.txt
--- a/app/src/main/assets/DevopsDoc/0.DevOps.txt	(revision 33974a2fda3bd9b55281f0b074a63deb654aa747)
+++ b/app/src/main/assets/DevopsDoc/0.DevOps.txt	(date 1739127114066)
@@ -1222,7 +1222,7 @@
 			- Install Kubernetes Software: by kubeadm	- There are three packages to be installed
 				+ kubeadm – installation and admin tool
 				+ kubelet – main Kubernetes linux service/daemon
-				+ kubectl – command line tool for controlling a Kubenetes cluster
+				+ kubectl – command line tool for controlling a Kubernetes cluster

 				# make sure you have the necessary tools
 				sudo apt-get install -y apt-transport-https ca-certificates curl
@@ -1250,8 +1250,8 @@
 				# sudo apt update -y && sudo apt upgrade -y
 			Tạo user devops và chuyển sang user devops
 				# adduser devops
+				# usermod -aG sudo devops
 				# su devops
-				# sudo usermod -aG sudo devops
 				# cd /home/devops
 			Tắt swap
 				# sudo swapoff -a
@@ -1294,23 +1294,27 @@
 			////////// Xem tiếp bài giảng để cài đặt K8s cluster phù hợp (xác định mô hình cụm cần cài đặt và reset cụm nếu cài không như mong muốn)
 			Mô hình đầu tiên:  1 master 2 worker
 				Thực hiện trên server k8s-master-1
-					# sudo kubeadm init						-> các node khác join vào master: 	sudo kubeadm join 192.168.56.70:6443 --token your_token --discovery-token-ca-cert-hash your_sha --control-plane --certificate-key your_cert
+					# sudo kubeadm init	--apiserver-advertise-address 192.168.56.70					-> các node khác join vào master: 	sudo kubeadm join 192.168.56.70:6443 --token your_token --discovery-token-ca-cert-hash your_sha --control-plane --certificate-key your_cert
+					// sudo kubeadm init --apiserver-advertise-address=192.168.56.70 --pod-network-cidr=192.168.0.0/16
 					# mkdir -p $HOME/.kube
 					# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
 					# sudo chown $(id -u):$(id -g) $HOME/.kube/config
-					# kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml		// cài network cho cụm
+					# kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml		// cài network cho cụm thì các node mới ready đc
 				Thực hiện join trên server k8s-master-2 và k8s master-3
 					# sudo kubeadm join 192.168.56.70:6443 --token your_token --discovery-token-ca-cert-hash your_sha
-
+					// config worker node: 	[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
+											[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
+											[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
+
 			Mô hình thứ hai: 3 master (worker) – xem video bài giảng để hiểu
 				Thực hiện trên server k8s-master-1
-					# sudo kubeadm init --control-plane-endpoint "192.168.1.111:6443" --upload-certs
+					# sudo kubeadm init --control-plane-endpoint "192.168.56.70:6443" --upload-certs
 					# mkdir -p $HOME/.kube
 					# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
 					# sudo chown $(id -u):$(id -g) $HOME/.kube/config
 					# kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml		// cài network cho cụm
 				Thực hiện trên server k8s-master-2 và k8s-master-3
-					# sudo kubeadm join 192.168.1.111:6443 --token your_token --discovery-token-ca-cert-hash your_sha --control-plane --certificate-key your_cert
+					# sudo kubeadm join 192.168.56.70:6443 --token your_token --discovery-token-ca-cert-hash your_sha --control-plane --certificate-key your_cert
 					# mkdir -p $HOME/.kube
 					# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
 					# sudo chown $(id -u):$(id -g) $HOME/.kube/config
@@ -1363,8 +1367,9 @@
 			+ View cluster vừa tạo -> Connected: google tụ động điền command kết nối -> Enter là vào đc k8s

 1. Triển khai project thực tế:			full stack back-front + db -> ra cách làm
-	1.1. Quy trình triển khai			// roadmap.sh		// search: kubernetes flow, 2 flow: architecture flow / deploy flow
+	1.1. Mô hình triển khai			// roadmap.sh		// search: kubernetes flow, 2 flow: architecture flow / deploy flow
 		+ Traffic -> Ingress [domain1, domain2] -> Service [service1, service2] -> [ Node: [pod1, pod2], Node[pod11, pod12] ]
+
 		+ File yaml:			// key: value				// k8s có support json nhưng ít dùng hơn yaml
 			- Cú pháp đơn giản
 			- Định dạng phong phú
@@ -1390,8 +1395,91 @@
 				+ metadata:		chứa thông tin liên quan đến resources:	name/labels/namespace/annotations
 				+ spec:			định nghĩa thông tin chi tiết cấu hình của resource tùy theo kind: [pod: spec của container in pod], [Deployment: spec của pod/template/jdk]

-		+ Namespace: chia nhóm node trong k8s để dễ quản lý và phân chia tài nguyên
-			-
+		+ Namespace:
+			- tổ chức và phân chia tài nguyên trong cụm k8s thành các nhóm nhỏ để dễ quản lý và vận hành, đảm bảo bảo mật, cũng như áp dụng các policy đồng nhất giữa các namespace:
+				+ kiểm soát lượng cpu, ram, tài nguyên trên từng project, nhóm
+				+ phân tách các môi trường: project, dev, qa, production (nhiều dự án làm việc trên 1 cụm)
+				+ dễ thực thi các policy bảo mật, kiểm soát quyền truy cập: hạn mức tài nguyên, ai/tài nguyên nào đc làm j trên ns đó
+
+			- mỗi cụm [cluster] có nhiều node, mỗi node có nhiều namespace	// nhiều cty chỉ tạo 1 cluster k8s với 1 node lớn, trong node đó chia nhiều namespace ->
+			- mỗi namespace là 1 project, mỗi project chứa nhiều pod, mỗi pod chứa nhiều container, mỗi ns nên có 1 file cấu hình .yaml
+			- luôn có 1 ns mặc định là defalut chứa pod của ng dùng, ngoài ra k8s tạo mặc định vài ns như: kube-node-lease, kube-public, kube-system
+
+			- kubectl create/delete ns "Namespace"	// init ns		-> 		kubectl apply/delete -f .yaml	// apply, delete cấu hình = file .yaml
+			- kubectl get ns						// list ns
+			- kubectl get pod	-> ns: default		// kubectl get pod --namespace Name
+
+			- Config: 					project1-ns.yaml
+				+ Config Namspace
+					apiVersion: v1
+					kind: Namspace
+					metadata:
+						name: project1
+				+ Giới hạn tài nguyên: project1-resource-quota.yaml
+					apiVersion: v1
+					kind:  ResourceQuota
+					metadata:
+						name: mem-cpu-quota
+						namespace: project1
+					spec:
+						hard:						# hardware
+							requests.cpu: "2"		# mã 2 cpus core
+							requests.memory: 4Gi	# mã 4GB mem
+
+	1.2. PP triển khai trên k8s: Tư duy ngược
+		- Nhu cầu -> Tool UI tổ chức -> trích xuất yaml -> Nghiên cứu syntax, format, value, template, options ... -> triển khai tương tự chuẩn format
+		- Tool UI tổ chức:				https://spacelift.io/blog/kubernetes-tools
+			+ commandline:	K9s
+			+ desktop:		Lends
+			+ website:		Rancher		-> cài rancher lên server để mn có thể truy cập vào, có phân quyền truy cập luôn, ko cần cài trên máy từng người
+		- Nghiên cứu syntax, format, value, template, options:
+			+ ConfigMap
+			+ Secret
+			+ Service
+			+ Monitor
+			+ Endpoint
+
+		- Rancher: triển khai, qly, giám sát nhiều cụm K8S trên các môi trường khác nhau: On-premise & Cloud
+			+ Ưu điểm:
+				- Tạo tài nguyên = UI
+				- Quản lý đc nhiều cụm K8s
+				- Phân quyền mạnh mẽ - based RBAC Kubernetes, custom đc cụ thể đến từng tài nguyên
+				- Hỗ trợ giám sát, warning cụm Kubernetes:
+				- Bảo mật tốt: authen = nhiều cách LDAP, github, webservice có mã hóa CLS, colaapj cụm, cô lập cụm
+				- được dử dụng trong các dự án lơn và môi trường dev/production
+			+ install:
+				- run Rancher by docker/docker-compose: 	cụm nhỏ, dev/staging
+				- run Rancher trực tiếp trên k8s = Helm:  	production, cụm to
+
+				- run Rancher by docker/docker-compose:
+					+ tạo server run rancher:	có thể lưu data trên 1 ổ cứng riêng rồi gán vào server -> server lỗi vần còn data
+					+ check list disk: lsblk	-> sda = ổ của OS, sdb = ổ rời bên ngoài [type: disk]
+					+ formart disk: mkfs.ext4 -m 0 /dev/sdb			// formart 100% và chỉ tạo 1 phân vùng
+					+ tạo folder để mount ổ mới:	mkdir /data
+					+ mount ổ vừa tạo vào folder /data server:	echo "/dev/sdb /data ext4 defaluts 0 0" | tee -a /etc/fstab -> mount -a	-> check: df -h
+
+					+ install docker: apt update -y && apt install docker.io docker-compose -y
+					+ check rancher version:	search rancher version matrix	-> cần check đúng highest version support của k8s hiện tại
+					+ install rancher =- docker-compose: mkdir /exHDD/rancher -> nano docker-compose.yaml	-> docker-com up -d		-> run rancher by IP/domain: change pass
+								version: '3'
+								services:
+								  rancher-server:
+									image: rancher/rancher:v2.9.2
+									container_name: rancher-server
+									restart: unless-stopped
+									ports:
+									  - "80:80"	`````````
+									  - "443:443"
+									volumes:
+									  - /exHDD/rancher/data:/var/lib/rancher
+									privileged: true
+						// volumes: /exHDD/rancher/data:/var/lib/rancher		-> mount volume của ranhcer /var/lib/rancher ra ổ cứng thứ 2 exHDD/rancher
+
+					+ rancher web -> import k8s: import existing -> cluster name -> add member role: Admin owner -> Create -> select command for unknown authority
+					+ master node: run command	-> download config .yaml && apply this file
+			+ using:
+				-
+
 2. Xây dựng công cụ dự án:				db, message, cache	-> dảm bảo data & tối ưu
 3. Giám sát và quản trị Kubernetes:		monitoring dự án, hạ tầng server, uptime, backup & restore trên cụm Kubernetes
 4.
@@ -1399,6 +1487,57 @@
 6.
 //////  KUBERNETES  ///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

+CẤU TRÚC POD.YAML TRONG KUBERNETES
+
+appversion: v1 					→ LOẠI API CỦA TÀI NGUYÊN (
+
+kind: pod 						→ LOẠI TÀI NGUYÊN (POD)
+
+metadata:
+	name: techops-pod 			→ Tên duy nhất của Pod
+	abels: 						→ Nhãn để tổ chức và chọn Podapp: techops → Nhãn ứng dụng, hữu ích cho bộ chọn (selectors)
+		tier: backend 			→ Xác định lớp ứng dụng
+
+spec:
+	containers: 				→ CONTAINER(S) CHẠY TRONG POD NÀY
+	- name: techops-container 	→ Tên của container
+	  image: nginx:1.23 		→ Docker image cho container
+	  ports:
+	  - containerPort: 80 		→ Cổng mà container mở nội bộ
+	  env:
+	  - name: ENV
+	    value: production 		→ Biến môi trường để cấu hình
+	  volumeMounts: 			→ CÁC VOLUMES ĐƯỢC GẮN VÀO TRONG CONTAINER
+	  - name: config-volume 	→ Volume được định nghĩa trong phần “volumes”
+		mountPath: /usr/share/nginx/html 	→ Điểm gắn kết (mount point) bên trong container
+
+	volumes:
+	- name: config-volume 		→ Tên của volume
+	  configMap:
+	    name: techops-config	→ Sử dụng ConfigMap làm nguồn và tham chiếu đến ConfigMap có tên “techops-config”
+
+	RESTARTPOLICY: ALWAYS 		→ CHÍNH SÁCH KHỞI ĐỘNG LẠI POD (ALWAYS, ONFAILURE HOẶC NEVER)
+
+	NODESELECTOR: 				→ LỊCH TRÌNH (SCHEDULE) POD TRÊN CÁC NODE CỤ THỂ
+	  disktype: ssd 			→ Chỉ chạy trên các node có nhãn “disktype=ssd”
+
+	TOLERATIONS: 				→ CHO PHÉP LÊN LỊCH TRÊN CÁC NODE BỊ "TAINT"
+	- key: "special-taint" 		→ Khóa taint cần chấp nhận
+	  operator: "Equal" 		→ Điều kiện khớp chính xác
+	  value: "true" 			→ Giá trị cần khớp
+	  effect: "NoExecute" 		→ Tác động: Không lên lịch (NoSchedule) hoặc đuổi khỏi node (NoExecute)
+
+	SECURITYCONTEXT: 			→ THIẾT LẬP BẢO MẬT Ở CẤP ĐỘ POD
+	  runAsUser: 1000 			→ Chạy container dưới user ID này
+	  runAsGroup: 3000 			→ Chạy container dưới group ID này
+	  fsGroup: 2000 			→ Nhóm sở hữu volumes đã gắn kết
+
+	INITCONTAINERS: 			→ CHẠY TRƯỚC KHI CONTAINER CHÍNH CHẠY
+	- name: init-techops 		→ Tên container khởi tạo
+	  image: busybox 			→ Image của container khởi tạo
+	  command: ["sh", "-c", "echo Init; sleep 5"] 	→ Câu lệnh khởi tạo
+///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
+

 https://www.baeldung.com/linux/virtualbox-ping-guest-machines

Index: app/src/main/assets/DevopsDoc/8.Kubernetes.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/app/src/main/assets/DevopsDoc/8.Kubernetes.txt b/app/src/main/assets/DevopsDoc/8.Kubernetes.txt
new file mode 100644
--- /dev/null	(date 1739114691254)
+++ b/app/src/main/assets/DevopsDoc/8.Kubernetes.txt	(date 1739114691254)
@@ -0,0 +1,127 @@
+1. Define
+	1.1. Define
+	1.2. Install
+		- Require: 						https://devopsedu.vn/courses/khoa-hoc-kubenetes-thuc-te/lesson/bai-5-trien-khai-kubernetes-cluster-tren-on-premise-2/
+			+ Deactive swap = off:	 	sudo swapoff -a					sudo sed -i '/swap.img/s/^/#/' /etc/fstab
+			+ Linux kernel modules:		overlay / br_netfilter			nano /etc/modules-load.d/containerd.conf			active: sudo modprobe overlay/br_netfilter
+			+ Network:					echo "net.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1" | sudo tee -a /etc/sysctl.d/kubernetes.conf
+			+ Container: 				containerd
+			+ kubelet kubeadm kubectl	need hold version after install
+		- Config:
+			+ /etc/apt/sources.list.d/kubernetes.list:				version k8s:		https://pkgs.k8s.io/core:/stable:/v1.30/deb/
+			+ /etc/apt/keyrings/kubernetes-apt-keyring.gpg			key of k8s:			https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key
+			+ /var/lib/etcd
+			+ /etc/kubernetes/manifests/							// kube-apiserver.yaml, kube-control-manager.yaml, kube-scheduler.yaml, etcd.yaml
+			+ /etc/kubernetes/admin.conf = $HOME/.kube/config
+				- apiVersion:
+				- clusters:
+					+ cluster:		server: https://10.0.2.15:6443			certificate-authority-data: sha
+				- contexts:
+					+ context
+				- kind:
+				- preferences:
+				- users:
+					+ name
+					+ client-certificate-data /  client-key-data
+		-
+	1.3. Fix - Change
+		- Change master node IP: 		192.168.56.50		-> need test more
+			+ kubectl drain --ignore-daemonsets													// Drain the node you want to change the IP address
+			+ nano /etc/kubernetes/manifests/kube-apiserver.yaml		/etc/kubernetes/admin.conf	// edit new IP			/etc/kubernetes/manifests/etcd.yaml
+			+ systemctl restart kube-apiserver kube-controller-manager/kube-scheduler				// Restart the Kubernetes control plane components
+			+ kubeadm token create --print-join-command												// Generate a new token for the worker nodes to join the cluster
+			+ kubeadm join IP:port --token master-token --discovery-token-ca-cert-hash sha-hash		// join worker node again
+
+			+ Master change IP: https://github.com/kubernetes/kubeadm/issues/338
+
+		- Wrong network adapter for VMs
+			+ 	1: lo: <LOOPBACK,UP,LOWER_UP>
+				2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
+					link/ether 08:00:27:bb:14:75 brd ff:ff:ff:ff:ff:ff
+					inet 10.0.2.15
+				3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
+					link/ether 08:00:27:fb:48:77 brd ff:ff:ff:ff:ff:ff
+					inet 100.0.0.1
+				4: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP>
+			+ sudo kubeadm init --apiserver-advertise-address=100.0.0.1 --pod-network-cidr=10.244.0.0/16
+			+ edit calico.yml:	--iface=eth1			// find: find ~/ -type f -name "calico.yaml"
+
+		- Worker node is not ready after join:		Network plugin returns error: cni plugin not initialized		-> Maybe VirtualBox using NAT IP, not Host-only IP
+			+ master node: kubectl -n kube-system apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.0/manifests/calico.yaml
+			+ worker node: kubectl describe node ks8-worker			->		see Addresses:InternalIP = 10.0.2.15 ->
+				- Fix IP: 		sudo nano /lib/systemd/system/kubelet.service.d/10-kubeadm.conf	--> add: --node-ip=192.168.56.60 for KUBELET_KUBECONFIG_ARGS --> sudo systemctl daemon-reload -> sudo systemctl restart kubelet
+				- Fix cni ...: 	scp /etc/cni/net.d/10-calico.conflist devops@192.168.56.60:/home/devops  ->  cp /home/devops/10-calico.conflist /etc/cni/net.d/  ->  systemctl restart kubelet
+
+		- Issue:
+			+ On kubemaster1:
+				enp0s8 is important, without this kubeadm takes enp0s3's address and then the second master can't connect due to lack of IP route
+
+				current_ip=$(/sbin/ip -o -4 addr list enp0s8 | awk '{print $4}' | cut -d/ -f1)
+				kubeadm init --apiserver-advertise-address=${current_ip} --pod-network-cidr 10.0.123.0/16
+				wget https://docs.projectcalico.org/manifests/calico.yaml
+				# I want to use different CIDR for Calico, thus the following change
+				sed -i 's/# - name: CALICO_IPV4POOL_CIDR/ - name: CALICO_IPV4POOL_CIDR/' calico.yaml
+				sed -i 's/#   value: "192.168.0.0/   value: "10.0.123.0/' calico.yaml
+			+ On kubemaster2:
+				echo "10.0.0.11  kubemaster1" >> /etc/hosts
+				JOIN_CMD=$(ssh -o StrictHostKeyChecking=no root@kubemaster1 "kubeadm token create --print-join-command")
+				echo $JOIN_CMD
+				$JOIN_CMD
+
+				The problem was that kubeadm was taking enp0s3 and putting it into the join command. Execution was hanging, as there was no IP route between the networks.
+				Hope this helps someone else in the future.
+2. Component
+	- Cluster
+		+ Master Mode = Control-plane:
+			- kube-api-server:			connect worker nodes vs master node
+			- etcd:						store: configs, status
+			- kube-scheduler:			supply workload
+			- kube-control-manager:		upgrade & fix
+			- kube-cloud-manager:		connect cloud-server-api
+		+ Worker Node:
+			- kube-proxy:	handle network
+			- kubenet:		manage pods
+			- node:
+				+ pod: container - container2
+				+ pod2
+		+ Worker Node2
+
+	- Container:
+		+ Types: Docker, Containerd
+		+ configs:							// /etc/containerd/config.toml
+			- root = "/var/lib/containerd"
+			- state = "/run/containerd"
+			- [grpc]						// address = "/run/containerd/containerd.sock"
+			- [metrics]						// address = ""
+			- [plugins]
+				+ [plugins."io.containerd.gc.v1.scheduler"]
+				+ [plugins."io.containerd.grpc.v1.cri"]			// sandbox_image = "registry.k8s.io/pause:3.8"		cdi_spec_dirs = ["/etc/cdi", "/var/run/cdi"]
+					- [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]			// SystemdCgroup = true
+3. Flow
+	3.1. Architect Flow:
+		- kube-api-server <---- worker node [kubenet/kube-proxy <---- pod]
+		- kube-api-server <---- etcd
+		- kube-api-server <---- kube-scheduler
+		- kube-api-server <---- kube-control-manager
+		- kube-api-server <---- kube-cloud-manager ---> kube-api-server
+
+	3.2. Deploy Flow:
+		- Traffic ---> Ingress [domain1 - domain2] ---> Service [service1 - service2] ---> Worker node [pod1 - pod2]
+
+4. Command
+	4.1. kubeadm
+	4.2. kubectl
+		- kubectl describe node nodeName			// Ready	- 	Addresses: IP		// cni plugin not initialized: fix= /etc/cni/net.d/
+		- kubectl get pods -o wide -n kube-system		kubectl get pods --all-namespaces		kubectl get nodes -o wide
+		- kubectl edit node <nodename>
+		- kubectl drain <node-name> 				// --ignore-daemonsets --delete-local-data
+		- kubectl delete node <node-name>
+	4.3. kubelet
+
+	- journalctl -u kubelet
+////////////////////////////////////////////////////////////////////////////////////////////
+
+////////////////////////////////////////////////////////////////////////////////////////////
+- Vấn đề loadbalancing với gRPC communication trong Kubernetes: https://viblo.asia/p/van-de-loadbalancing-voi-grpc-communication-trong-kubernetes-part-1-zOQJwKybJMP?fbclid=IwY2xjawIR6sdleHRuA2FlbQIxMAABHTa2C24faou0oyYlJ4IKShViiEwVGeKa4FucO794f7HV8E5svUYSnBCcHw_aem_8X_KLvVJxxVR_Josoml2Fw
+- Database CI/CD and Security at Scale: https://www.bytebase.com/?fbclid=IwY2xjawIR6mNleHRuA2FlbQIxMAABHQbaPmMPQklUXzFriHi2_mwAipc2ARUvA5nXH7F07M4aHh92oKdJzWuyZQ_aem__TpfeZfSgIbko7eLjUDSSA
+////////////////////////////////////////////////////////////////////////////////////////////
diff --git a/app/src/main/assets/DevopsDoc/12.Vault.txt b/app/src/main/assets/DevopsDoc/12.Vault.txt
new file mode 100644
diff --git a/app/src/main/assets/DevopsDoc/9.Zabbix.txt b/app/src/main/assets/DevopsDoc/9.Zabbix.txt
new file mode 100644
diff --git a/app/src/main/assets/DevopsDoc/11.Gitops.txt b/app/src/main/assets/DevopsDoc/11.Gitops.txt
new file mode 100644
